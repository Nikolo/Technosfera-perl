Веб краулер с использованием AnyEvent или Coro
================================
* На вход подаётся URL и фактор паралльности.

Требования к роботу:

* Собрать с сайта все ссылки на уникальные страницы
* Для каждой найденной ссылки выполнить запрос HEAD
* Если содержимое text/html, то выполнить запрос GET
* Для каждой скачанной страницы запомнить её размер
* Если страниц более `1000`, собрать максимум `1000` уникальных ссылок
* Относительные ссылки превращать в абсолютные.
* Из сылок должны быть вырезаны теги привязки 
* Обрабатывать (запрашивать) только ссылки, которые начинаются на URL
* Вывести `Top-10` страниц по размеру и суммарный размер всех страниц

Модули, которые могут помочь в решении: `URI`, `AnyEvent::HTTP`, `Coro::LWP`, `Web::Query`. `Web::Query` допустимо использовать только как парсер документов, но не как инструмент для скачивания

Файлы:
* lib/Crawler.pm - место для реализации crawler-а

Тест:
```bash
perl Makefile.PL && make test
```